{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scifa\\Anaconda2\\envs\\python3\\lib\\site-packages\\h5py\\__init__.py:72: UserWarning: h5py is running against HDF5 1.10.2 when it was built against 1.10.3, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "### LOAD PACKAGES\n",
    "import os\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "np.random.seed(1337) # for reproducibility\n",
    "import keras\n",
    "import pandas as pd\n",
    "import csv\n",
    "from pandas import read_csv, DataFrame\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.layers import Dense, Flatten, TimeDistributed, LSTM, GlobalAveragePooling1D, Dropout, Activation\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import np_utils\n",
    "from random import shuffle\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfPackets = 32 #Number of strings of network data used for each dataset\n",
    "numOfBytes = 784  #Number of bytes used from each string of network data\n",
    "numOfClasses = 15 #Total number of classes to be classified. (Number of different labels)\n",
    "\n",
    "labelList = []\n",
    "dataList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Assigns each class/subclass an integer label.\n",
    "\"\"\"\n",
    "def gNum(type):\n",
    "    typeTuple = [(\"Google+\",0),(\"GoogleEarth\",1),(\"GoogleMap\",2),(\"GoogleMusic\",3),(\"GooglePlay\",4),(\"Hangouts\",5),(\"WebMail_Gmail\",6),(\"YouTube\",7),(\"Google_Common\",8),(\"GoogleAnalytics\",9),(\"GoogleSearch\",10),(\"GoogleAdsense\",11),(\"TCPConnect\",12),(\"HTTP\",13),(\"HTTPS\",14)]\n",
    "    dic = dict(typeTuple)\n",
    "    return dic[type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Takes a string of network packet data and converts bytes of hex data into normalized floats. Returns a list of floats for each string of bytes.\n",
    "\"\"\"\n",
    "def pad_and_convert(hexStr):\n",
    "    if len(hexStr) < 400:\n",
    "        hexStr += '00' * (400-len(hexStr))\n",
    "    else:\n",
    "        hexStr = hexStr[:400]\n",
    "    return [float(int(hexStr[i]+hexStr[i+1], 16)/255) for i in range(0, 400, 2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Reads in a directory of files and extracts the needed network data and labels.\n",
    "\"\"\"\n",
    "def getFiles():\n",
    "    global dataList\n",
    "    global labelList\n",
    "    os.chdir(\"/Users/scifa/Documents/ai_research/files/\")\n",
    "    for directories in os.listdir(os.getcwd()):\n",
    "        dir = os.path.join('/Users/scifa/Documents/ai_research/files/', directories)\n",
    "        os.chdir(dir)\n",
    "        for subdirectories in os.listdir(os.getcwd()):\n",
    "            subdir = os.path.join(dir, subdirectories)\n",
    "            subdirSplit = subdirectories.split(\"_\")\n",
    "            deviceType = subdirSplit[1]\n",
    "            os.chdir(subdir)\n",
    "            for filename in os.listdir(subdir):\n",
    "                d = []\n",
    "                l = []\n",
    "                if os.path.isfile:\n",
    "                    file = (os.path.join(subdir, filename))\n",
    "                    with open(file) as tsv:\n",
    "                        splitFilename = file.split(\"-\")\n",
    "                        underscoreSplitFilename = filename.split(\"_\")\n",
    "                        fileSubclass = splitFilename[1]\n",
    "                        dotSplitFilename = (underscoreSplitFilename[6]).split(\".\")\n",
    "                        fileFlowstate = filename[-15]\n",
    "                        d = []\n",
    "                        #if directories == fileSubclass:\n",
    "                        okSubclasses = ['HTTP', 'GoogleEarth', 'GoogleMap', 'Google_Common', 'Google+', 'GoogleSearch', 'GoogleAnalytics', 'TCPConnect', 'HTTPS', 'WebMail_Gmail', 'Hangouts', 'GooglePlay', 'YouTube', 'GoogleMusic', 'GoogleAdsense']\n",
    "                        if fileSubclass in okSubclasses:\n",
    "                            count = 0\n",
    "                            pktStr = \"\"\n",
    "                            totalPktStr = \"\"\n",
    "                            for line in csv.reader(tsv, dialect=\"excel-tab\"):\n",
    "                                if count < 4:\n",
    "                                    count = count + 1\n",
    "                                    pktStr = line[3]\n",
    "                                    totalPktStr = totalPktStr + pktStr[0:100]\n",
    "                                    #dataList.append(pad_and_convert(line[3]))\n",
    "                            dataList.append(pad_and_convert(totalPktStr))\n",
    "                            labelList.append(fileSubclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open directory and extract the needed network data and labels\n",
    "getFiles()\n",
    "global dataList\n",
    "global labelList\n",
    "#Convert label into an int value\n",
    "intLabelList = []\n",
    "for i in labelList:\n",
    "    intLabelList.append(gNum(i))\n",
    "\n",
    "#Determine size of test and train data\n",
    "totalSizeOfDataset = len(dataList)\n",
    "testSize = int(.10 * totalSizeOfDataset)\n",
    "trainSize = int(.90 * totalSizeOfDataset)\n",
    "\n",
    "#Initial shuffle of data\n",
    "dataList, intLabelList = sklearn.utils.shuffle(dataList, intLabelList, random_state = 0)\n",
    "\n",
    "#Break data into training and test\n",
    "x_train = []\n",
    "x_test = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "for i in range(0, trainSize):\n",
    "    y_train.append(intLabelList[i])\n",
    "    x_train.append(dataList[i])\n",
    "for i in range(trainSize, totalSizeOfDataset):\n",
    "    y_test.append(intLabelList[i])\n",
    "    x_test.append(dataList[i])\n",
    "\n",
    "#Fix the data so the CNN can read it\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "y_train = np_utils.to_categorical(y_train, numOfClasses)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np_utils.to_categorical(y_test, numOfClasses)\n",
    "y_test = np.array(y_test)\n",
    "x_train = np.expand_dims(x_train, axis=2)\n",
    "x_test = np.expand_dims(x_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM model\n",
    "activation = 'relu'\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv1D(512, strides=2, input_shape=x_train.shape[1:], activation=activation, kernel_size=4, padding='same')))\n",
    "model.add(TimeDistributed(MaxPooling1D(data_format='channels_first')))\n",
    "model.add(TimeDistributed(Conv1D(256, strides=1, activation=activation, kernel_size=2, padding='same')))\n",
    "model.add(TimeDistributed(MaxPooling1D(data_format='channels_first')))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(TimeDistributed(Dense(128, activation=activation)))\n",
    "model.add(TimeDistributed(Dense(128, activation=activation)))\n",
    "model.add(TimeDistributed(Dense(32, activation=activation)))\n",
    "model.add(TimeDistributed(Dense(32, activation=activation)))\n",
    "model.add(LSTM(20, return_sequences=False, name=\"lstm_layer\"))\n",
    "model.add(Dense(numOfClasses, activation='softmax'))\n",
    "print(model.summary())\n",
    "opt = keras.optimizers.Adam(lr=0.004)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "#Enable early stopping and model saving\n",
    "os.chdir(\"/Users/scifa/Documents/ai_research\")\n",
    "callbacks_list = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='best_model.{epoch:02d}-{val_loss:.2f}.h5',\n",
    "        monitor='val_loss', save_best_only=True),\n",
    "    keras.callbacks.EarlyStopping(monitor='acc', patience=1)\n",
    "]\n",
    "\n",
    "#Run the model\n",
    "result = model.fit(x_train, y_train, verbose=1, epochs=50, batch_size=128, validation_data=(x_test, y_test), callbacks=callbacks_list, shuffle = True)\n",
    "\n",
    "#Generating Confusion Matrix\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "cm =confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "np.set_printoptions(threshold=np.nan, linewidth=100)\n",
    "print(cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
